{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fake_news_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPDvzpw90pkIlZQLGIW+fhY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thedataninja1786/Machine-Learning/blob/main/Fake_news_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zOdVGDkfKrp"
      },
      "source": [
        "# Fake news classifier using an LSTM network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95a50tdHcufm"
      },
      "source": [
        "##Importing the necessary modules\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhV10Mmqctmw"
      },
      "source": [
        "!pip install wandb \n",
        "from time import time \n",
        "import re \n",
        "import sys \n",
        "import matplotlib.pyplot as plt \n",
        "from matplotlib import rcParams\n",
        "import seaborn as sns\n",
        "import nltk \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perception_tagger')\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "import itertools \n",
        "import datetime \n",
        "import pprint \n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.feature_extraction.text import CountVectorizer \n",
        "from sklearn.model_selection import train_test_split\n",
        "import os \n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "from google.colab import drive \n",
        "from sklearn.utils import shuffle\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "from keras.preprocessing import sequence \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation \n",
        "from keras.layers import Embedding, LSTM \n",
        "from keras.layers import Conv1D, Flatten, MaxPooling1D\n",
        "from keras.preprocessing.sequence import pad_sequences \n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.preprocessing import text \n",
        "import tensorflow as tf\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqDaqp-Bc4Z8"
      },
      "source": [
        "##Loading the data and creating labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ug0ZxN_khRX5"
      },
      "source": [
        "data = pd.read_csv('/content/drive/My Drive/news.csv')\n",
        "data = data[['text','label']].copy()\n",
        "\n",
        "data['text'] = data['text'].astype(str)\n",
        "\n",
        "data['label'] = data['label'].apply(lambda x: 1 if x== 'FAKE' else 0)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_zhFJtIroGTV",
        "outputId": "e0223172-7966-47d7-94dc-15c29c1b89cf"
      },
      "source": [
        "fake_news = (data['label'] == 1).sum()\n",
        "true_news = (data['label'] == 0).sum()\n",
        "\n",
        "f'The dataset comprises of {fake_news} articles of fake news and {true_news} articles of true news.'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The dataset comprises of 3164 articles of fake news and 3171 articles of true news.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeLApSSFdABD"
      },
      "source": [
        "##Data preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWCQxpAqibFC"
      },
      "source": [
        "import re \n",
        "import string \n",
        "\n",
        "def remove_URL(text):\n",
        "  url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "  return url.sub(r\"\",text)\n",
        "\n",
        "def remove_punct(text):\n",
        "  translator = str.maketrans(\"\",\"\",string.punctuation)\n",
        "  return text.translate(translator)\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]\n",
        "  return \" \".join(filtered_words)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnN8PsPuifgA"
      },
      "source": [
        "data['text'] = data['text'] .apply(lambda x : remove_URL(x))\n",
        "data['text']  = data['text'] .apply(lambda x : remove_punct(x))\n",
        "data['text']  = data['text'] .apply(lambda x : remove_stopwords(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJqL6S9ri3hm"
      },
      "source": [
        "#Counting unique words\n",
        "from collections import Counter \n",
        "\n",
        "def count_words(text_col):\n",
        "  count = Counter()\n",
        "  for text in text_col.values:\n",
        "    for word in text.split():\n",
        "      count[word] += 1 \n",
        "  return count \n",
        "\n",
        "counter = count_words(data['text'])\n",
        "counter.most_common()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R9fwk2-cdTR"
      },
      "source": [
        "##Creating the train and validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1D1yI9L4jKgE"
      },
      "source": [
        "train_size = int(data.shape[0] * 0.8)\n",
        "\n",
        "train_df = data[:train_size]\n",
        "val_df = data[train_size:]\n",
        "\n",
        "#Split text and labels and convert to numpy arrays \n",
        "train_sentences = train_df['text'].to_numpy()\n",
        "train_labels = train_df['label'].to_numpy()\n",
        "val_sentences = val_df['text'].to_numpy()\n",
        "val_labels = val_df['label'].to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5Z9zcWZjjG5"
      },
      "source": [
        "train_sentences.shape, val_sentences.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYqoD1VvdIJp"
      },
      "source": [
        "##Creating the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRIncmIqjq51"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "#Vectorize a text corpus by turning each sentence into a sequence of integers \n",
        "tokenizer = Tokenizer(num_words = len(counter))\n",
        "tokenizer.fit_on_texts(train_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZPLA4crksjR"
      },
      "source": [
        "#Now each word has a unique index \n",
        "word_index = tokenizer.word_index\n",
        "word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7VtMombktKR"
      },
      "source": [
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "val_sequences = tokenizer.texts_to_sequences(val_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loUVBnUokh7z",
        "outputId": "fa7b52ee-bac1-4e48-9102-e33159c68313"
      },
      "source": [
        "#Apply paddying so all the sequences have the same length \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#max number of words in a sequence \n",
        "max_length = 100\n",
        "\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding= 'post', truncating= 'post')\n",
        "val_padded = pad_sequences(val_sequences, maxlen=max_length, padding= 'post', truncating= 'post')\n",
        "train_padded.shape , val_padded.shape "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5068, 100), (1267, 100))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMA9smmBfd7Y"
      },
      "source": [
        "##Creating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN-n3Rndk4XQ",
        "outputId": "f0b4fc62-02fb-4968-be37-e3b0ead50a3a"
      },
      "source": [
        "#Creating the LSTM model \n",
        "import keras\n",
        "from tensorflow.keras import layers \n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(layers.Embedding(len(counter),32,input_length = max_length))\n",
        "model.add(layers.LSTM(64, dropout = 0.1))\n",
        "model.add(layers.Dense(1,activation = 'sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 32)           3507392   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 64)                24832     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 3,532,289\n",
            "Trainable params: 3,532,289\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8C_jwbgk7Fx"
      },
      "source": [
        "loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "optim = keras.optimizers.Adam(lr = 0.0001)\n",
        "metrics = ['accuracy']\n",
        "\n",
        "model.compile(loss = loss , optimizer= optim, metrics = metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf5C_vNuk-iu",
        "outputId": "93fc4d35-5144-4b3f-dfcc-fc783077c854"
      },
      "source": [
        "model.fit(train_padded,train_labels,epochs=25,validation_data=(val_padded,val_labels), verbose = 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "159/159 - 14s - loss: 0.6920 - accuracy: 0.5298 - val_loss: 0.6879 - val_accuracy: 0.5714\n",
            "Epoch 2/25\n",
            "159/159 - 14s - loss: 0.6218 - accuracy: 0.6523 - val_loss: 0.4083 - val_accuracy: 0.8414\n",
            "Epoch 3/25\n",
            "159/159 - 14s - loss: 0.3297 - accuracy: 0.8777 - val_loss: 0.2769 - val_accuracy: 0.8966\n",
            "Epoch 4/25\n",
            "159/159 - 14s - loss: 0.1515 - accuracy: 0.9546 - val_loss: 0.2361 - val_accuracy: 0.9061\n",
            "Epoch 5/25\n",
            "159/159 - 14s - loss: 0.0769 - accuracy: 0.9801 - val_loss: 0.2233 - val_accuracy: 0.9203\n",
            "Epoch 6/25\n",
            "159/159 - 14s - loss: 0.0399 - accuracy: 0.9903 - val_loss: 0.2244 - val_accuracy: 0.9203\n",
            "Epoch 7/25\n",
            "159/159 - 14s - loss: 0.0272 - accuracy: 0.9951 - val_loss: 0.3055 - val_accuracy: 0.9116\n",
            "Epoch 8/25\n",
            "159/159 - 14s - loss: 0.0197 - accuracy: 0.9966 - val_loss: 0.3431 - val_accuracy: 0.9084\n",
            "Epoch 9/25\n",
            "159/159 - 14s - loss: 0.0150 - accuracy: 0.9976 - val_loss: 0.3363 - val_accuracy: 0.9084\n",
            "Epoch 10/25\n",
            "159/159 - 14s - loss: 0.0129 - accuracy: 0.9972 - val_loss: 0.3837 - val_accuracy: 0.9124\n",
            "Epoch 11/25\n",
            "159/159 - 14s - loss: 0.0107 - accuracy: 0.9984 - val_loss: 0.4404 - val_accuracy: 0.9069\n",
            "Epoch 12/25\n",
            "159/159 - 14s - loss: 0.0159 - accuracy: 0.9976 - val_loss: 0.5550 - val_accuracy: 0.8927\n",
            "Epoch 13/25\n",
            "159/159 - 14s - loss: 0.0222 - accuracy: 0.9961 - val_loss: 0.4177 - val_accuracy: 0.9013\n",
            "Epoch 14/25\n",
            "159/159 - 14s - loss: 0.0076 - accuracy: 0.9990 - val_loss: 0.3652 - val_accuracy: 0.9155\n",
            "Epoch 15/25\n",
            "159/159 - 14s - loss: 0.0083 - accuracy: 0.9988 - val_loss: 0.4166 - val_accuracy: 0.9021\n",
            "Epoch 16/25\n",
            "159/159 - 14s - loss: 0.0065 - accuracy: 0.9988 - val_loss: 0.5479 - val_accuracy: 0.8950\n",
            "Epoch 17/25\n",
            "159/159 - 14s - loss: 0.0056 - accuracy: 0.9994 - val_loss: 0.5334 - val_accuracy: 0.9045\n",
            "Epoch 18/25\n",
            "159/159 - 14s - loss: 0.0118 - accuracy: 0.9980 - val_loss: 0.4218 - val_accuracy: 0.9084\n",
            "Epoch 19/25\n",
            "159/159 - 14s - loss: 0.0361 - accuracy: 0.9939 - val_loss: 0.6028 - val_accuracy: 0.8942\n",
            "Epoch 20/25\n",
            "159/159 - 15s - loss: 0.0069 - accuracy: 0.9990 - val_loss: 0.4049 - val_accuracy: 0.9100\n",
            "Epoch 21/25\n",
            "159/159 - 16s - loss: 0.0042 - accuracy: 0.9996 - val_loss: 0.4667 - val_accuracy: 0.9077\n",
            "Epoch 22/25\n",
            "159/159 - 15s - loss: 0.0041 - accuracy: 0.9996 - val_loss: 0.4850 - val_accuracy: 0.9084\n",
            "Epoch 23/25\n",
            "159/159 - 14s - loss: 0.0038 - accuracy: 0.9996 - val_loss: 0.5144 - val_accuracy: 0.9069\n",
            "Epoch 24/25\n",
            "159/159 - 15s - loss: 0.0038 - accuracy: 0.9996 - val_loss: 0.5241 - val_accuracy: 0.9021\n",
            "Epoch 25/25\n",
            "159/159 - 15s - loss: 0.0048 - accuracy: 0.9992 - val_loss: 0.5181 - val_accuracy: 0.8990\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f575d2a9668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0t7BqPEqNd8"
      },
      "source": [
        "predictions = model.predict(train_padded)\n",
        "predictions = [1 if p > 0.5 else 0 for p in predictions]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Spe1Ob1_pK6x"
      },
      "source": [
        "print(train_sentences[0:5])\n",
        "\n",
        "print(train_labels[0:5])\n",
        "\n",
        "print(predictions[0:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad7wppMQgHgH"
      },
      "source": [
        "##Saving the tokenizer and the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP_-0srCqZ35"
      },
      "source": [
        "import pickle \n",
        "\n",
        "with open('/content/drive/My Drive/tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "model.save('/content/drive/My Drive/Fake_news_classifier')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKOL4V81gcLx"
      },
      "source": [
        "##Testing the model on some new data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGFGqv6mM-pk"
      },
      "source": [
        "#Importing the necessary modules \n",
        "import requests \n",
        "from bs4 import BeautifulSoup as bs \n",
        "from urllib.parse import urljoin \n",
        "import time \n",
        "import pandas as pd \n",
        "import re \n",
        "from datetime import datetime as dt\n",
        "import datetime\n",
        "import os \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDyGlW3nNWq7"
      },
      "source": [
        "#Function for getting the article's data \n",
        "def get_article_info(url):\n",
        "  r = requests.get(url)\n",
        "  soup = bs(r.content)\n",
        "  articles = {}\n",
        "  articles['title'] = soup.find(class_=\"StandardHeader__title\").get_text()\n",
        "  articles['published_date'] = soup.find(class_=\"PublicationTime__date\").get_text()\n",
        "  articles['content'] = soup.find(class_=\"Body__content\").get_text()\n",
        "  return articles \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uENIyPtkP_-Y"
      },
      "source": [
        "#Function for getting the article's links from the website \n",
        "\n",
        "r = requests.get('http://www.thedailybeast.com/politics.html') #website that publishes political articles\n",
        "soup = bs(r.content)\n",
        "links = []\n",
        "urls = soup.find_all(class_=\"GridStory__title-link\")\n",
        "for url in urls:\n",
        "  links.append(url.find(class_='TrackingLink')['href'])\n",
        "\n",
        "links\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUe7xJE2gR0c"
      },
      "source": [
        "#Getting all the necessary info for all the articles \n",
        "list_of_articles = []\n",
        "for link in links:\n",
        "  list_of_articles.append(get_article_info(link))\n",
        "\n",
        "articles = pd.DataFrame(articles)\n",
        "articles.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqT6W8O6g4iC"
      },
      "source": [
        "#Making prediction with the new articles \n",
        "contents = articles['content'].to_list()\n",
        "\n",
        "contents_seq = np.array(tokenizer.texts_to_sequences(contents))\n",
        "\n",
        "contents_pad = pad_sequences(contents_seq,maxlen = 100, padding = 'post')\n",
        "\n",
        "predictions_2 = model.predict(contents_pad)\n",
        "predictions_2 = [1 if p > 0.5 else 0 for p in predictions_2]\n",
        "predictions_2 = ['FAKE' if p ==1 else 'TRUE' for p in predictions_2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "1MHdcXn0hXQm",
        "outputId": "2dfd9409-9147-463d-a2ca-0f4b6e79576b"
      },
      "source": [
        "\n",
        "ls = []\n",
        "for i, content in enumerate(contents):\n",
        "  ds = {'Article' : content , 'Label' : predictions_2[i]}\n",
        "  ls.append(ds)\n",
        "\n",
        "classification = pd.DataFrame(ls)\n",
        "classification"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Article</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>As it seeks to overturn the election results a...</td>\n",
              "      <td>TRUE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Let’s say you’re standing next to some railroa...</td>\n",
              "      <td>TRUE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>President Donald Trump’s administration signal...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A former cabinet member once said to me that t...</td>\n",
              "      <td>TRUE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>It’s been three weeks since Donald Trump lost ...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Antony Blinken, a longtime aide to President-e...</td>\n",
              "      <td>TRUE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Recalling the arduous 2008 path that ultimatel...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>To understand the wrongness, ignorance, and ju...</td>\n",
              "      <td>TRUE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>One of the first things Joe Biden can do to he...</td>\n",
              "      <td>TRUE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>The number screamed at me and I actually wante...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Article Label\n",
              "0  As it seeks to overturn the election results a...  TRUE\n",
              "1  Let’s say you’re standing next to some railroa...  TRUE\n",
              "2  President Donald Trump’s administration signal...  FAKE\n",
              "3  A former cabinet member once said to me that t...  TRUE\n",
              "4  It’s been three weeks since Donald Trump lost ...  FAKE\n",
              "5  Antony Blinken, a longtime aide to President-e...  TRUE\n",
              "6  Recalling the arduous 2008 path that ultimatel...  FAKE\n",
              "7  To understand the wrongness, ignorance, and ju...  TRUE\n",
              "8  One of the first things Joe Biden can do to he...  TRUE\n",
              "9  The number screamed at me and I actually wante...  FAKE"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    }
  ]
}